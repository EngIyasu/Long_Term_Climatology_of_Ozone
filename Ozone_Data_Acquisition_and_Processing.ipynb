{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ozone_Data_Acquisition_and_Processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMOIwnx0h7Brj7zXog+UHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EngIyasu/Long_Term_Climatology_of_Ozone/blob/master/Ozone_Data_Acquisition_and_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFAtkaRq8a06"
      },
      "source": [
        "## Raw Ozone Data Acquisition and Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT5rvmpZ8ix3"
      },
      "source": [
        "This part one notebook acquires and manipulates the hourly raw ozone data from the pre-generated files of dataset available at [US Environmenal Protection Agency (EPA)](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw). It automatically requests, acquires, and process the raw zipped data from the EPA website, and saves the processed and filtered data in CSV format for further statistical analysis presented in part two of this repository. In this demonstration, the script uses the Houston Site, Texas, United States, as this city is among the cities in the United States with the worst ozone pollution. To filter the data for the site, the \"State Code\", \"County Code\", and \"Site Number\" informations are used associated with that site specificaion. After concatenating the data from different years, the compact and refined data was saved to local compute as CSV format. This approach allows the user to deal with very big data in efficient way and computationally less expensive data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaWpNuxN8n8h"
      },
      "source": [
        "### Required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOJXdaII8vHX"
      },
      "source": [
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "import io\n",
        "from pathlib import Path\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwyEXanz8xjy"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDkBZydo8990",
        "outputId": "7dfeac56-58ae-4aa0-9379-7e9fb966add1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2psnR6H9BCE"
      },
      "source": [
        "### Clone the Repository "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gLLuPgM9KSU",
        "outputId": "c498693f-d155-405a-f006-f46ac9599c65"
      },
      "source": [
        "!git clone https://github.com/EngIyasu/Long_Term_Climatology_of_Ozone.git #clone the repository to access files from there\n",
        "!git -C Long_Term_Climatology_of_Ozone/ pull #pull the latest"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Long_Term_Climatology_of_Ozone' already exists and is not an empty directory.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEfzhmDh9Rrc"
      },
      "source": [
        "### Setting Working Directory in Google Drive Folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wet2iehD9dI9"
      },
      "source": [
        "os.chdir(\"/content/Long_Term_Climatology_of_Ozone\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tWhvSUh9jRV"
      },
      "source": [
        "### Acquiring, filtering, and processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGsOueWd9spB"
      },
      "source": [
        "epa_base_dir = 'https://aqs.epa.gov/aqsweb/airdata/hourly_44201_'\n",
        "epa_dir_list = [f'{epa_base_dir}{i}.zip' for i in range(2005,2020)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs5pzT7H9usb",
        "outputId": "29d9de84-0708-4fda-cbd7-41afb14fb491"
      },
      "source": [
        "%%time\n",
        "state_code = 48\n",
        "county_code = 201\n",
        "site_num = 24\n",
        "\n",
        "dfs = []\n",
        "for myzip in epa_dir_list:\n",
        "    with urlopen(myzip) as req:\n",
        "        zip_file = ZipFile(io.BytesIO(req.read()))\n",
        "        csv_file = f\"{Path(myzip).stem}.csv\"\n",
        "        df = pd.read_csv(zip_file.open(csv_file),low_memory=False)\n",
        "        \n",
        "        dff = df.loc[(df['State Code'] == state_code) & (df[\"County Code\"] == county_code) & (df[\"Site Num\"] == site_num)]       \n",
        "       \n",
        "        df_filtered = pd.DataFrame(data={\n",
        "                   \"State Code\": dff[\"State Code\"].values,\n",
        "                   \"County Code\": dff[\"County Code\"].values,\n",
        "                   \"Site Num\": dff[\"Site Num\"].values,\n",
        "                   \"ozone_ppm\": dff[\"Sample Measurement\"].values,\n",
        "                   \"Date GMT\": dff[\"Date GMT\"], \n",
        "                   \"Time GMT\": dff[\"Time GMT\"], \n",
        "        })\n",
        "        \n",
        "        df_filtered[\"Date Time GMT\"] = pd.DatetimeIndex(df_filtered[\"Date GMT\"] + \" \" + df_filtered[\"Time GMT\"])\n",
        "        df_filtered_mod = df_filtered.drop(['Date GMT', 'Time GMT'], axis=1)\n",
        "        \n",
        "        # get Beltsville data\n",
        "        dfs.append(df_filtered_mod)\n",
        "        \n",
        "    \n",
        "        print ('Uncompressing data file ...', csv_file)\n",
        "        del df, df_filtered, dff, zip_file, df_filtered_mod\n",
        "        \n",
        "df_final = pd.concat(dfs)\n",
        "df_final.reset_index(drop = True)\n",
        "df_final.to_csv(\"./data/Houston.csv\",index=False, encoding='utf-8-sig')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uncompressing data file ... hourly_44201_2005.csv\n",
            "Uncompressing data file ... hourly_44201_2006.csv\n",
            "Uncompressing data file ... hourly_44201_2007.csv\n",
            "Uncompressing data file ... hourly_44201_2008.csv\n",
            "Uncompressing data file ... hourly_44201_2009.csv\n",
            "Uncompressing data file ... hourly_44201_2010.csv\n",
            "Uncompressing data file ... hourly_44201_2011.csv\n",
            "Uncompressing data file ... hourly_44201_2012.csv\n",
            "Uncompressing data file ... hourly_44201_2013.csv\n",
            "Uncompressing data file ... hourly_44201_2014.csv\n",
            "Uncompressing data file ... hourly_44201_2015.csv\n",
            "Uncompressing data file ... hourly_44201_2016.csv\n",
            "Uncompressing data file ... hourly_44201_2017.csv\n",
            "Uncompressing data file ... hourly_44201_2018.csv\n",
            "Uncompressing data file ... hourly_44201_2019.csv\n",
            "CPU times: user 7min 13s, sys: 43.2 s, total: 7min 57s\n",
            "Wall time: 21min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USPBuTuu9zxw"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}