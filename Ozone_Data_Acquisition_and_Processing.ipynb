{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Ozone Data Acquisition and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part one notebook acquires and manipulates the hourly raw ozone data from the pre-generated files of dataset available at [US Environmenal Protection Agency (EPA)](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw). It automatically requests, acquires, and process the raw zipped data from the EPA website, and saves the processed and filtered data in CSV format for further statistical analysis presented in part two of this repository. In this demonstration, the script uses the Houston Site, Texas, United States, as this city is among the cities in the United States with the worst ozone pollution. To filter the data for the site, the \"State Code\", \"County Code\", and \"Site Number\" informations are used associated with that site specificaion. After concatenating the data from different years, the compact and refined data was saved to local compute as CSV format. This approach allows the user to deal with very big data in efficient way and computationally less expensive data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import io\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquiring, filtering, and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_base_dir = 'https://aqs.epa.gov/aqsweb/airdata/hourly_44201_'\n",
    "epa_dir_list = [f'{epa_base_dir}{i}.zip' for i in range(2005,2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (0,17,23) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressing data file ... hourly_44201_2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (17,23) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressing data file ... hourly_44201_2006.csv\n",
      "Uncompressing data file ... hourly_44201_2007.csv\n",
      "Uncompressing data file ... hourly_44201_2008.csv\n",
      "Uncompressing data file ... hourly_44201_2009.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressing data file ... hourly_44201_2010.csv\n",
      "Uncompressing data file ... hourly_44201_2011.csv\n",
      "Uncompressing data file ... hourly_44201_2012.csv\n",
      "Uncompressing data file ... hourly_44201_2013.csv\n",
      "Uncompressing data file ... hourly_44201_2014.csv\n",
      "Uncompressing data file ... hourly_44201_2015.csv\n",
      "Uncompressing data file ... hourly_44201_2016.csv\n",
      "Uncompressing data file ... hourly_44201_2017.csv\n",
      "Uncompressing data file ... hourly_44201_2018.csv\n",
      "Uncompressing data file ... hourly_44201_2019.csv\n",
      "CPU times: user 4min 34s, sys: 29.3 s, total: 5min 4s\n",
      "Wall time: 15min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "state_code = 48\n",
    "county_code = 201\n",
    "site_num = 24\n",
    "\n",
    "dfs = []\n",
    "for myzip in epa_dir_list:\n",
    "    with urlopen(myzip) as req:\n",
    "        zip_file = ZipFile(io.BytesIO(req.read()))\n",
    "        csv_file = f\"{Path(myzip).stem}.csv\"\n",
    "        df = pd.read_csv(zip_file.open(csv_file))\n",
    "        \n",
    "        dff = df.loc[(df['State Code'] == state_code) & (df[\"County Code\"] == county_code) & (df[\"Site Num\"] == site_num)]       \n",
    "       \n",
    "        df_filtered = pd.DataFrame(data={\n",
    "                   \"State Code\": dff[\"State Code\"].values,\n",
    "                   \"County Code\": dff[\"County Code\"].values,\n",
    "                   \"Site Num\": dff[\"Site Num\"].values,\n",
    "                   \"ozone_ppm\": dff[\"Sample Measurement\"].values,\n",
    "                   \"Date GMT\": dff[\"Date GMT\"], \n",
    "                   \"Time GMT\": dff[\"Time GMT\"], \n",
    "        })\n",
    "        \n",
    "        df_filtered[\"Date Time GMT\"] = pd.DatetimeIndex(df_filtered[\"Date GMT\"] + \" \" + df_filtered[\"Time GMT\"])\n",
    "        df_filtered_mod = df_filtered.drop(['Date GMT', 'Time GMT'], axis=1)\n",
    "        \n",
    "        # get Beltsville data\n",
    "        dfs.append(df_filtered_mod)\n",
    "        \n",
    "    \n",
    "        print ('Uncompressing data file ...', csv_file)\n",
    "        del df, df_filtered, dff, zip_file, df_filtered_mod\n",
    "        \n",
    "df_final = pd.concat(dfs)\n",
    "df_final.reset_index(drop = True)\n",
    "df_final.to_csv(\"./data/Houston.csv\",index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
